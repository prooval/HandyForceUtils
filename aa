async executeMigration(options: MigrationQueryOptions): Promise<MigrationResult> {
    const startTime = new Date();
    const overallMigrationResult: MigrationResult = {
        success: true,
        recordsProcessed: 0,
        recordsFailedValidation: 0,
        recordsFailed: 0,
        errors: [],
        warnings: [],
        startTime,
        endTime: startTime
    };

    try {
        // Get target connection first
        await this.getTargetUserId();

        if (!(await this.beforeMigration())) {
            throw new Error("Pre-migration checks failed");
        }

        const query = await this.buildQuery(options);
        const batchSize = this.config.batchSize || 2000;

        this.logger.logInfo(`BaseMigrationObject -> executeMigration - source dataset query generated: ${query}; batchSize: ${batchSize}; now about to trigger bulk API query`);
        this.logMemoryUsage();

        // Check if file-based staging is enabled
        if (this.config.useFileBasedStaging) {
            return await this.executeFileBasedMigration(query, batchSize, overallMigrationResult);
        } else {
            return await this.executeInMemoryMigration(query, batchSize, overallMigrationResult);
        }

    } catch (error) {
        this.logger.logError(error as Error);
        this.logger.logInfo(`Inside executeMigration, encountered an exception for the entire job!`);

        overallMigrationResult.success = false;
        overallMigrationResult.errors.push(error as Error);
        overallMigrationResult.endTime = new Date();

        return overallMigrationResult;
    }
}

private async executeFileBasedMigration(query: string, batchSize: number, overallMigrationResult: MigrationResult): Promise<MigrationResult> {
    // Stage 1: Export data to CSV file
    const stagingFilename = await this.stageDataToCsv(query);
    
    this.logger.logInfo(`BaseMigrationObject -> executeFileBasedMigration - data staged to file: ${stagingFilename}`);
    this.logMemoryUsage();

    // Stage 2: Process CSV file in batches using streaming
    let currentBatch: any[] = [];
    let currentBatchNumber = 0;
    const self = this;

    // Create a transform stream for processing records
    const processStream = new Transform({
        objectMode: true,
        async transform(chunk: any, encoding: string, callback: TransformCallback) {
            try {
                // Early out for max record limit
                if (self.config.maxRecordLimit && overallMigrationResult.recordsProcessed >= self.config.maxRecordLimit) {
                    this.push(null);
                    return callback();
                }

                currentBatch.push(chunk);

                if (currentBatch.length >= batchSize) {
                    try {
                        self.logger.logInfo(`Processing file-based batch ${currentBatchNumber} with ${currentBatch.length} records`);
                        
                        const currentBatchResult = await self.processIndividualMigrationBatch(currentBatch, currentBatchNumber);

                        // Update overall results
                        overallMigrationResult.recordsProcessed += currentBatchResult.recordsProcessed;
                        overallMigrationResult.recordsFailedValidation += currentBatchResult.recordsFailedValidation;
                        overallMigrationResult.recordsFailed += currentBatchResult.recordsFailed;

                        // Log progress
                        self.logger.logBatchProgress(self.config.sourceObject, currentBatchResult);
                        self.logger.logProgress(self.config.sourceObject, overallMigrationResult.recordsProcessed);
                        self.logger.logInfo(`Completed file-based batch ${currentBatchNumber}`);

                        currentBatch = [];
                        currentBatchNumber++;

                        // Clear lookup cache
                        self.lookupCache.clear();
                        self.logMemoryUsage();

                    } catch (error) {
                        overallMigrationResult.success = false;
                        overallMigrationResult.errors.push(error as Error);
                        overallMigrationResult.recordsFailed += currentBatch.length;
                        
                        self.logger.logInfo(`Exception in file-based batch ${currentBatchNumber}`);
                        self.logger.logError(error as Error);
                    }
                }

                callback();
            } catch (error: any) {
                callback(error);
            }
        }
    });

    return new Promise((resolve, reject) => {
        this.getFileStream(stagingFilename)
            .pipe(processStream)
            .on('finish', async () => {
                try {
                    // Process remaining records
                    if (currentBatch.length > 0) {
                        this.logger.logInfo(`Processing final file-based batch with ${currentBatch.length} records`);
                        
                        const finalBatchResult = await this.processIndividualMigrationBatch(currentBatch, currentBatchNumber);
                        overallMigrationResult.recordsProcessed += finalBatchResult.recordsProcessed;
                        overallMigrationResult.recordsFailedValidation += finalBatchResult.recordsFailedValidation;
                        overallMigrationResult.recordsFailed += finalBatchResult.recordsFailed;
                    }

                    // Clean up staging file
                    await this.cleanupStagingFile(stagingFilename);

                    // Record the migration end time
                    overallMigrationResult.endTime = new Date();

                    const finalResult: MigrationResult = await this.afterMigration(overallMigrationResult);
                    this.logger.logInfo(`Finished file-based processing; finalResult: ${JSON.stringify(finalResult)}`);

                    resolve(finalResult);
                } catch (error) {
                    reject(error);
                }
            })
            .on('error', (error) => {
                reject(error);
            });
    });
}

private async executeInMemoryMigration(query: string, batchSize: number, overallMigrationResult: MigrationResult): Promise<MigrationResult> {
    // Hit Bulk API v2 --- this awaits until a job is created
    const queryResultStream = await this.sourceCom.bulk2.query(query, {
        pollTimeout: this.pollTimeout,
        pollInterval: this.pollInterval
    });

    this.logger.logInfo(`BaseMigrationObject -> executeMigration - bulk API query completed`);

    // Create a promise that will resolve when the entire records are collected and processed
    const recordCollectionPromise = new Promise<SalesforceRecord<TSource>[]>((resolve, reject) => {
        // Collect all records into memory
        const allRecords: SalesforceRecord<TSource>[] = [];

        queryResultStream
            .on("record", (record: SalesforceRecord<TSource>) => {
                const thisRecord = record as SalesforceRecord<TSource>;

                // Edge case: sometimes the cast produces junk records that have "Id" as the id, which is
                // not a real Id. If we see such records, we will ignore them
                if (thisRecord.Id !== 'Id') {
                    allRecords.push(thisRecord);
                }
            })
            .on("error", (err) => {
                this.logger.logInfo(`Exception inside queryResultStream -> on error event`);
                this.logger.logError(err as Error);
                reject(err);
            })
            .on("end", () => {
                this.logger.logInfo(`Collected ${allRecords.length} records from source`);
                resolve(allRecords);
            });
    });

    this.logMemoryUsage();

    // Wait for all records to be collected
    const collectedRecords = await recordCollectionPromise;
    this.logger.logInfo(`BaseMigrationObject -> executeMigration - finished collecting all records from the stream`);
    this.logMemoryUsage();

    const totalBatches = Math.ceil(collectedRecords.length / batchSize);
    this.logger.logInfo(`Processing ${totalBatches} batches sequentially; total collected records: ${collectedRecords.length}`);

    // STEP 2: Process all batches sequentially for optimal memory usage and simplicity of processing
    // Process each batch one at a time
    for (let i = 0; i < collectedRecords.length; i += batchSize) {
        const currentBatchNumber = Math.floor(i / batchSize);
        const endIndex = Math.min(i + batchSize, collectedRecords.length);

        // Slice up the batch of records
        this.logMemoryUsage();
        const currentBatch = collectedRecords.slice(i, endIndex);
        this.logMemoryUsage();

        try {
            this.logger.logInfo(`Starting batch ${currentBatchNumber}/${totalBatches} with ${currentBatch.length} records`);

            const currentBatchResult = await this.processIndividualMigrationBatch(currentBatch, currentBatchNumber);

            // Update overall results
            overallMigrationResult.recordsProcessed += currentBatchResult.recordsProcessed;
            overallMigrationResult.recordsFailedValidation += currentBatchResult.recordsFailedValidation;
            overallMigrationResult.recordsFailed += currentBatchResult.recordsFailed;

            // Log progress
            this.logger.logBatchProgress(this.config.sourceObject, currentBatchResult);
            this.logger.logProgress(this.config.sourceObject, overallMigrationResult.recordsProcessed);
            this.logger.logInfo(`Completed batch ${currentBatchNumber}/${totalBatches}`);

            // Batch is now complete and can be garbage collected
            // The batch variable will be cleared in the next iteration

        } catch (error) {
            overallMigrationResult.success = false;
            overallMigrationResult.errors.push(error as Error);
            overallMigrationResult.recordsFailed = currentBatch.length;

            this.logger.logInfo(`Exception inside executeMigration -> singleBatchPromise while processing batch number ${currentBatchNumber}/${totalBatches}`);
            this.logger.logError(error as Error);

            // Continue with next batch or break if you want to stop on first error
            // break; // Uncomment to stop on first error
        }
        finally {
            // Clear the look up cache for this batch
            this.lookupCache.clear();
            this.logger.logInfo(`Cleared look up cache after finishing processing for batch number ${currentBatchNumber}/${totalBatches}`);
            this.logMemoryUsage();
        }
    }

    // Clear the original array to free up memory
    collectedRecords.length = 0;

    // Record the migration end time
    overallMigrationResult.endTime = new Date();

    const finalResult: MigrationResult = await this.afterMigration(overallMigrationResult);
    this.logger.logInfo(`Finished processing all batches; finalResult: ${JSON.stringify(finalResult)}`);

    return finalResult;
}

private async stageDataToCsv(query: string): Promise<string> {
    const timestamp = new Date().toISOString().split('T')[0];
    const stagingDir = './cdp-data/bulkcsv-export';
    const filename = `${this.config.sourceObject}-export-${timestamp}.csv`;
    const fullPath = `${stagingDir}/${filename}`;

    // Ensure staging directory exists
    const fs = require('fs');
    fs.mkdirSync(stagingDir, { recursive: true });

    // Create write stream for CSV file
    const csvFileOut = fs.createWriteStream(fullPath);

    // Execute bulk query and pipe to CSV
    const recordStream = await this.sourceCom.bulk2.query(query, {
        pollInterval: this.pollInterval || 5000,
        pollTimeout: this.pollTimeout || 3000000
    });

    return new Promise((resolve, reject) => {
        recordStream.stream().pipe(csvFileOut)
            .on('finish', () => {
                this.logger.logInfo(`Data successfully staged to CSV file: ${fullPath}`);
                resolve(fullPath);
            })
            .on('error', (error) => {
                this.logger.logError(`Error staging data to CSV: ${error}`);
                reject(error);
            });
    });
}

private getFileStream(filename: string) {
    const fs = require('fs');
    const csv = require('csv-parser');
    
    return fs.createReadStream(filename).pipe(csv({
        headers: true,
        skipEmptyLines: true
    }));
}

private async cleanupStagingFile(filename: string): Promise<void> {
    try {
        const fs = require('fs').promises;
        await fs.unlink(filename);
        this.logger.logInfo(`Cleaned up staging file: ${filename}`);
    } catch (error) {
        this.logger.logError(`Error cleaning up staging file ${filename}: ${error}`);
        // Don't throw error for cleanup failures
    }
}
